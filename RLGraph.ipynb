{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "from itertools import combinations\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# =========================================\n",
        "# GRAPH UTILITY FUNCTIONS\n",
        "# =========================================\n",
        "\n",
        "def create_graph(n):\n",
        "    \"\"\"Create a random graph with n nodes.\"\"\"\n",
        "    G = nx.fast_gnp_random_graph(n, 0.5)\n",
        "    return G\n",
        "\n",
        "def create_state_pairs(n):\n",
        "    \"\"\"Create a pair of initial and target graphs.\"\"\"\n",
        "    G1 = create_graph(n)\n",
        "    G2 = create_graph(n)\n",
        "\n",
        "    # Ensure G2 is not empty\n",
        "    while G2.number_of_edges() == 0:\n",
        "        G2 = create_graph(n)\n",
        "\n",
        "    return [G1, G2]\n",
        "\n",
        "def local_complementation(G, node):\n",
        "    \"\"\"Apply local complementation on a given node.\"\"\"\n",
        "    G_copy = G.copy()\n",
        "    neighbors = list(G_copy.neighbors(node))\n",
        "\n",
        "    for i in range(len(neighbors)):\n",
        "        for j in range(i+1, len(neighbors)):\n",
        "            ni, nj = neighbors[i], neighbors[j]\n",
        "            if G_copy.has_edge(ni, nj):\n",
        "                G_copy.remove_edge(ni, nj)\n",
        "            else:\n",
        "                G_copy.add_edge(ni, nj)\n",
        "\n",
        "    return G_copy\n",
        "\n",
        "def calculate_cost(operations):\n",
        "    \"\"\"Calculate cost of operations (edge ops cost 10, local ops cost 1).\"\"\"\n",
        "    edge_ops = sum(1 for op, _ in operations if op == \"edge\")\n",
        "    local_ops = sum(1 for op, _ in operations if op == \"local\")\n",
        "    return edge_ops * 10 + local_ops\n",
        "\n",
        "def apply_operation(graph, op_type, param):\n",
        "    \"\"\"Apply a single operation to a graph.\"\"\"\n",
        "    G_copy = graph.copy()\n",
        "    if op_type == \"edge\":\n",
        "        i, j = param\n",
        "        if G_copy.has_edge(i, j):\n",
        "            G_copy.remove_edge(i, j)\n",
        "        else:\n",
        "            G_copy.add_edge(i, j)\n",
        "    else:  # op_type == \"local\"\n",
        "        G_copy = local_complementation(G_copy, param)\n",
        "    return G_copy\n",
        "\n",
        "def graph_difference(G1, G2):\n",
        "    \"\"\"Calculate difference between graphs.\"\"\"\n",
        "    adj1 = nx.to_numpy_array(G1)\n",
        "    adj2 = nx.to_numpy_array(G2)\n",
        "    return np.sum(np.abs(adj1 - adj2))\n",
        "\n",
        "def estimate_min_solution_cost(initial_graph, target_graph):\n",
        "    \"\"\"\n",
        "    Estimate the theoretical minimum solution cost based on graph properties.\n",
        "    \"\"\"\n",
        "    # Calculate the graph difference (number of different edges)\n",
        "    diff = graph_difference(initial_graph, target_graph)\n",
        "\n",
        "    # In the best case, we might be able to change multiple edges with\n",
        "    # a single local complementation (LC), which costs 1\n",
        "    # Assume optimistically that an LC can change up to 3 edges\n",
        "    edges_via_lc = diff // 3\n",
        "    direct_edge_ops = diff - (edges_via_lc * 3)\n",
        "\n",
        "    # Calculate estimated minimum cost\n",
        "    # Each LC costs 1, each edge operation costs 10\n",
        "    min_cost = edges_via_lc + (direct_edge_ops * 10)\n",
        "\n",
        "    # Put a reasonable lower bound based on empirical evidence\n",
        "    return max(min_cost, 3)  # At least 3 as a safe lower bound\n",
        "\n",
        "# =========================================\n",
        "# GRAPH FEATURE EXTRACTION\n",
        "# =========================================\n",
        "\n",
        "def graph_to_features(G, target_G=None, max_nodes=25):\n",
        "    \"\"\"Convert a NetworkX graph to a feature matrix and adjacency matrix.\"\"\"\n",
        "    n = G.number_of_nodes()\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adj_matrix = nx.to_numpy_array(G)\n",
        "\n",
        "    # Create feature matrix with basic node features\n",
        "    # Features: degree, clustering coefficient\n",
        "    features = np.zeros((n, 3))\n",
        "    for i in range(n):\n",
        "        features[i, 0] = G.degree(i) / n  # Normalized degree\n",
        "        features[i, 1] = nx.clustering(G, i) if G.degree(i) > 1 else 0  # Clustering coefficient\n",
        "\n",
        "        # If target graph is provided, add a feature indicating if node has different connections\n",
        "        if target_G:\n",
        "            target_adj = nx.to_numpy_array(target_G)\n",
        "            features[i, 2] = np.sum(np.abs(adj_matrix[i] - target_adj[i])) / n\n",
        "\n",
        "    # Create target mask if needed (for larger graphs, could be used to focus on specific nodes)\n",
        "    target_mask = np.ones(n)\n",
        "\n",
        "    # Pad to max_nodes for consistent tensor sizes\n",
        "    if n < max_nodes:\n",
        "        adj_pad = np.zeros((max_nodes, max_nodes))\n",
        "        adj_pad[:n, :n] = adj_matrix\n",
        "\n",
        "        feat_pad = np.zeros((max_nodes, features.shape[1]))\n",
        "        feat_pad[:n] = features\n",
        "\n",
        "        mask_pad = np.zeros(max_nodes)\n",
        "        mask_pad[:n] = target_mask\n",
        "\n",
        "        return feat_pad, adj_pad, mask_pad, n\n",
        "\n",
        "    return features, adj_matrix, target_mask, n\n",
        "\n",
        "# =========================================\n",
        "# NEURAL NETWORK\n",
        "# =========================================\n",
        "\n",
        "class GraphTransformNet(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=64, max_nodes=25):\n",
        "        super(GraphTransformNet, self).__init__()\n",
        "\n",
        "        self.max_nodes = max_nodes\n",
        "\n",
        "        # Graph embedding layers\n",
        "        self.node_embed = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # For processing adjacency information\n",
        "        self.graph_conv1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.graph_conv2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Combine current and target graph information\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Action-specific heads\n",
        "        self.local_head = nn.Linear(hidden_dim, 1)  # Score for local complementation on each node\n",
        "        self.edge_head = nn.Linear(hidden_dim * 2, 1)  # Score for toggling each edge\n",
        "\n",
        "        # Value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_features, adjacency, target_features, target_adjacency, mask, num_nodes):\n",
        "        \"\"\"Forward pass to compute action probabilities and state value.\"\"\"\n",
        "        batch_size = node_features.size(0)\n",
        "\n",
        "        # Initial node embeddings\n",
        "        node_embeddings = self.node_embed(node_features)\n",
        "        target_embeddings = self.node_embed(target_features)\n",
        "\n",
        "        # Simple graph convolution operations\n",
        "        # Multiply node features by adjacency matrix to aggregate neighborhood info\n",
        "        neighbor_features = torch.bmm(adjacency, node_embeddings)\n",
        "        node_embeddings = F.relu(self.graph_conv1(node_embeddings + neighbor_features))\n",
        "\n",
        "        neighbor_features = torch.bmm(adjacency, node_embeddings)\n",
        "        node_embeddings = F.relu(self.graph_conv2(node_embeddings + neighbor_features))\n",
        "\n",
        "        # Same for target graph\n",
        "        target_neighbor_features = torch.bmm(target_adjacency, target_embeddings)\n",
        "        target_embeddings = F.relu(self.graph_conv1(target_embeddings + target_neighbor_features))\n",
        "\n",
        "        target_neighbor_features = torch.bmm(target_adjacency, target_embeddings)\n",
        "        target_embeddings = F.relu(self.graph_conv2(target_embeddings + target_neighbor_features))\n",
        "\n",
        "        # Global graph representation (simple mean pooling)\n",
        "        graph_embed = torch.sum(node_embeddings * mask.unsqueeze(-1), dim=1) / torch.sum(mask, dim=1, keepdim=True)\n",
        "        target_graph_embed = torch.sum(target_embeddings * mask.unsqueeze(-1), dim=1) / torch.sum(mask, dim=1, keepdim=True)\n",
        "\n",
        "        # Combine current and target information\n",
        "        combined = torch.cat([graph_embed, target_graph_embed], dim=1)\n",
        "\n",
        "        # Broadcast combined info back to nodes\n",
        "        combined_node = self.combine(torch.cat([\n",
        "            node_embeddings,\n",
        "            target_embeddings\n",
        "        ], dim=2))\n",
        "\n",
        "        # Get local complementation logits for each node\n",
        "        local_logits = self.local_head(combined_node).squeeze(-1)\n",
        "\n",
        "        # Create edge representation for all possible edges\n",
        "        edge_logits = torch.zeros((batch_size, self.max_nodes, self.max_nodes)).to(node_features.device)\n",
        "\n",
        "        # Calculate edge logits\n",
        "        for i in range(self.max_nodes):\n",
        "            for j in range(i+1, self.max_nodes):\n",
        "                if i < num_nodes and j < num_nodes:\n",
        "                    # Concatenate node embeddings for the edge\n",
        "                    edge_embed = torch.cat([\n",
        "                        combined_node[:, i],\n",
        "                        combined_node[:, j]\n",
        "                    ], dim=1)\n",
        "\n",
        "                    # Get edge toggle logit\n",
        "                    edge_logit = self.edge_head(edge_embed).squeeze(-1)\n",
        "                    edge_logits[:, i, j] = edge_logit\n",
        "                    edge_logits[:, j, i] = edge_logit  # Symmetric\n",
        "\n",
        "        # Mask out invalid nodes/edges\n",
        "        mask_2d = mask.unsqueeze(2) * mask.unsqueeze(1)\n",
        "        local_logits = local_logits * mask\n",
        "        edge_logits = edge_logits * mask_2d\n",
        "\n",
        "        # Get state value\n",
        "        value = self.value_head(combined)\n",
        "\n",
        "        return local_logits, edge_logits, value\n",
        "\n",
        "# =========================================\n",
        "# ENVIRONMENT\n",
        "# =========================================\n",
        "\n",
        "class GraphTransformationEnv:\n",
        "    def __init__(self, initial_graph, target_graph):\n",
        "        self.initial_graph = initial_graph\n",
        "        self.target_graph = target_graph\n",
        "        self.current_graph = initial_graph.copy()\n",
        "        self.n_nodes = initial_graph.number_of_nodes()\n",
        "        self.operations_history = []\n",
        "        self.initial_difference = graph_difference(initial_graph, target_graph)\n",
        "        self.step_count = 0\n",
        "        self.max_steps = self.n_nodes * 3  # Allow more steps for larger graphs\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_graph = self.initial_graph.copy()\n",
        "        self.operations_history = []\n",
        "        self.step_count = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        # Return current graph and target graph\n",
        "        return self.current_graph, self.target_graph\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Take a step in the environment by applying the given action.\n",
        "        Enhanced with sophisticated rewards to guide toward optimal solutions.\n",
        "        \"\"\"\n",
        "        op_type, param = action\n",
        "\n",
        "        # Store the previous graph state to compare changes\n",
        "        prev_graph = self.current_graph.copy()\n",
        "\n",
        "        # Apply the operation\n",
        "        new_graph = apply_operation(self.current_graph, op_type, param)\n",
        "\n",
        "        # Record the operation\n",
        "        self.operations_history.append((op_type, param))\n",
        "\n",
        "        # Calculate reward based on graph similarity improvement\n",
        "        prev_diff = graph_difference(self.current_graph, self.target_graph)\n",
        "        new_diff = graph_difference(new_graph, self.target_graph)\n",
        "\n",
        "        # Enhanced reward for moving closer to target graph\n",
        "        diff_improvement = prev_diff - new_diff\n",
        "        reward = diff_improvement * 5  # Stronger gradient\n",
        "\n",
        "        # Operation cost penalties (standard)\n",
        "        if op_type == \"edge\":\n",
        "            reward -= 10  # High cost for edge operations\n",
        "        else:\n",
        "            reward -= 1   # Low cost for local complementation\n",
        "\n",
        "        # Check if the action is a local complementation that efficiently changes multiple edges\n",
        "        if op_type == \"local\":\n",
        "            node = param\n",
        "            affected_edges = self._count_affected_edges_by_LC(prev_graph, new_graph)\n",
        "            if affected_edges > 2:  # If LC affected multiple edges efficiently\n",
        "                # Bonus reward proportional to effectiveness\n",
        "                reward += affected_edges * 2\n",
        "                # Extra bonus if the changes move us toward the target\n",
        "                if diff_improvement > 0:\n",
        "                    reward += affected_edges * 3\n",
        "\n",
        "        # Update current graph\n",
        "        self.current_graph = new_graph\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Check if target reached (exact match)\n",
        "        target_reached = (new_diff == 0)\n",
        "\n",
        "        # Check if we're very close to target (for partial credit)\n",
        "        very_close = (new_diff <= 2)  # Within 1-2 edges of target\n",
        "\n",
        "        # Step limit reached\n",
        "        step_limit_reached = (self.step_count >= self.max_steps)\n",
        "\n",
        "        # Determine if episode is done\n",
        "        done = target_reached or step_limit_reached\n",
        "\n",
        "        # Reward bonuses for completion\n",
        "        if target_reached:\n",
        "            # Base completion bonus\n",
        "            reward += 100\n",
        "\n",
        "            # Reward inversely proportional to solution cost\n",
        "            solution_cost = calculate_cost(self.operations_history)\n",
        "            reward += 500 / (solution_cost + 1)  # +1 to avoid division by zero\n",
        "\n",
        "            # Bonus for solutions with fewer operations (parsimony)\n",
        "            reward += 20 / (len(self.operations_history) + 1)\n",
        "\n",
        "            # Special bonus for finding very efficient solutions\n",
        "            if solution_cost < 15:  # Empirically determined threshold for \"very good\" solutions\n",
        "                reward += 100\n",
        "            elif solution_cost < 25:  # Good solutions\n",
        "                reward += 50\n",
        "        elif very_close:\n",
        "            reward += 20   # Partial bonus for getting very close\n",
        "\n",
        "        # Add penalties for long episodes without success\n",
        "        if step_limit_reached and not target_reached:\n",
        "            reward -= 30   # Penalty for failing to reach target within steps\n",
        "\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _count_affected_edges_by_LC(self, prev_graph, new_graph):\n",
        "        \"\"\"\n",
        "        Count how many edges were affected by a local complementation operation.\n",
        "        \"\"\"\n",
        "        prev_edges = set(prev_graph.edges())\n",
        "        new_edges = set(new_graph.edges())\n",
        "\n",
        "        # Edges that were added or removed\n",
        "        added_edges = new_edges - prev_edges\n",
        "        removed_edges = prev_edges - new_edges\n",
        "\n",
        "        # Total number of affected edges\n",
        "        return len(added_edges) + len(removed_edges)\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        valid_actions = []\n",
        "\n",
        "        # Local complementation on each node\n",
        "        for node in range(self.n_nodes):\n",
        "            # Skip isolated nodes (no effect)\n",
        "            if self.current_graph.degree(node) > 0:\n",
        "                valid_actions.append((\"local\", node))\n",
        "\n",
        "        # Edge toggle for each pair\n",
        "        for i, j in combinations(range(self.n_nodes), 2):\n",
        "            valid_actions.append((\"edge\", (i, j)))\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "# =========================================\n",
        "# EXPERIENCE REPLAY BUFFER\n",
        "# =========================================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.Experience = namedtuple('Experience',\n",
        "                                    ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = self.Experience(state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, k=min(batch_size, len(self.buffer)))\n",
        "        return experiences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# =========================================\n",
        "# RL AGENT\n",
        "# =========================================\n",
        "\n",
        "class GraphTransformAgent:\n",
        "    def __init__(self, input_dim=3, hidden_dim=64, max_nodes=25, lr=0.001, gamma=0.99, device='cpu'):\n",
        "        self.device = device\n",
        "        self.max_nodes = max_nodes\n",
        "        self.model = GraphTransformNet(input_dim, hidden_dim, max_nodes).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 16\n",
        "        self.memory = ReplayBuffer(capacity=50000)\n",
        "\n",
        "    def select_action(self, env, current_graph, target_graph):\n",
        "        valid_actions = env.get_valid_actions()\n",
        "\n",
        "        # Random action with probability epsilon\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Convert graphs to tensors\n",
        "            features, adjacency, mask, n_nodes = graph_to_features(current_graph, target_graph, self.max_nodes)\n",
        "            target_features, target_adjacency, _, _ = graph_to_features(target_graph, current_graph, self.max_nodes)\n",
        "\n",
        "            # Add batch dimension and convert to tensors\n",
        "            features = torch.FloatTensor(features).unsqueeze(0).to(self.device)\n",
        "            adjacency = torch.FloatTensor(adjacency).unsqueeze(0).to(self.device)\n",
        "            target_features = torch.FloatTensor(target_features).unsqueeze(0).to(self.device)\n",
        "            target_adjacency = torch.FloatTensor(target_adjacency).unsqueeze(0).to(self.device)\n",
        "            mask = torch.FloatTensor(mask).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Get action scores\n",
        "            local_logits, edge_logits, _ = self.model(features, adjacency, target_features, target_adjacency, mask, n_nodes)\n",
        "\n",
        "            # Score each valid action\n",
        "            action_scores = {}\n",
        "            for action in valid_actions:\n",
        "                if action[0] == \"local\":\n",
        "                    node = action[1]\n",
        "                    score = local_logits[0, node].item()\n",
        "                else:  # \"edge\"\n",
        "                    i, j = action[1]\n",
        "                    score = edge_logits[0, i, j].item()\n",
        "\n",
        "                action_scores[action] = score\n",
        "\n",
        "            # Select action with highest score\n",
        "            best_action = max(action_scores.items(), key=lambda x: x[1])[0]\n",
        "            return best_action\n",
        "\n",
        "    def _prepare_batch(self, experiences):\n",
        "        \"\"\"Convert batch of experiences to tensors for training.\"\"\"\n",
        "        # Initialize tensors\n",
        "        batch_size = len(experiences)\n",
        "        features_batch = []\n",
        "        adjacency_batch = []\n",
        "        mask_batch = []\n",
        "        target_features_batch = []\n",
        "        target_adjacency_batch = []\n",
        "        actions_batch = []\n",
        "        rewards = []\n",
        "        next_features_batch = []\n",
        "        next_adjacency_batch = []\n",
        "        next_mask_batch = []\n",
        "        next_target_features_batch = []\n",
        "        next_target_adjacency_batch = []\n",
        "        dones = []\n",
        "        n_nodes_batch = []\n",
        "\n",
        "        # Process each experience\n",
        "        for exp in experiences:\n",
        "            current_graph, target_graph = exp.state\n",
        "            next_current_graph, next_target_graph = exp.next_state\n",
        "\n",
        "            # Current state\n",
        "            features, adjacency, mask, n_nodes = graph_to_features(current_graph, target_graph, self.max_nodes)\n",
        "            target_features, target_adjacency, _, _ = graph_to_features(target_graph, current_graph, self.max_nodes)\n",
        "\n",
        "            # Next state\n",
        "            next_features, next_adjacency, next_mask, _ = graph_to_features(next_current_graph, next_target_graph, self.max_nodes)\n",
        "            next_target_features, next_target_adjacency, _, _ = graph_to_features(next_target_graph, next_current_graph, self.max_nodes)\n",
        "\n",
        "            # Add to batches\n",
        "            features_batch.append(features)\n",
        "            adjacency_batch.append(adjacency)\n",
        "            mask_batch.append(mask)\n",
        "            target_features_batch.append(target_features)\n",
        "            target_adjacency_batch.append(target_adjacency)\n",
        "\n",
        "            next_features_batch.append(next_features)\n",
        "            next_adjacency_batch.append(next_adjacency)\n",
        "            next_mask_batch.append(next_mask)\n",
        "            next_target_features_batch.append(next_target_features)\n",
        "            next_target_adjacency_batch.append(next_target_adjacency)\n",
        "\n",
        "            actions_batch.append(exp.action)\n",
        "            rewards.append(exp.reward)\n",
        "            dones.append(float(exp.done))\n",
        "            n_nodes_batch.append(n_nodes)\n",
        "\n",
        "        # Convert to tensors\n",
        "        features_tensor = torch.FloatTensor(np.array(features_batch)).to(self.device)\n",
        "        adjacency_tensor = torch.FloatTensor(np.array(adjacency_batch)).to(self.device)\n",
        "        mask_tensor = torch.FloatTensor(np.array(mask_batch)).to(self.device)\n",
        "        target_features_tensor = torch.FloatTensor(np.array(target_features_batch)).to(self.device)\n",
        "        target_adjacency_tensor = torch.FloatTensor(np.array(target_adjacency_batch)).to(self.device)\n",
        "\n",
        "        next_features_tensor = torch.FloatTensor(np.array(next_features_batch)).to(self.device)\n",
        "        next_adjacency_tensor = torch.FloatTensor(np.array(next_adjacency_batch)).to(self.device)\n",
        "        next_mask_tensor = torch.FloatTensor(np.array(next_mask_batch)).to(self.device)\n",
        "        next_target_features_tensor = torch.FloatTensor(np.array(next_target_features_batch)).to(self.device)\n",
        "        next_target_adjacency_tensor = torch.FloatTensor(np.array(next_target_adjacency_batch)).to(self.device)\n",
        "\n",
        "        rewards_tensor = torch.FloatTensor(rewards).to(self.device)\n",
        "        dones_tensor = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        return {\n",
        "            'current_state': (features_tensor, adjacency_tensor, target_features_tensor, target_adjacency_tensor, mask_tensor, n_nodes_batch),\n",
        "            'actions': actions_batch,\n",
        "            'rewards': rewards_tensor,\n",
        "            'next_state': (next_features_tensor, next_adjacency_tensor, next_target_features_tensor, next_target_adjacency_tensor, next_mask_tensor, n_nodes_batch),\n",
        "            'dones': dones_tensor\n",
        "        }\n",
        "\n",
        "    def _learn(self):\n",
        "        \"\"\"Update model based on experiences.\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        # Sample experiences\n",
        "        experiences = self.memory.sample(self.batch_size)\n",
        "        batch = self._prepare_batch(experiences)\n",
        "\n",
        "        # Unpack batch\n",
        "        features, adjacency, target_features, target_adjacency, mask, n_nodes = batch['current_state']\n",
        "        actions = batch['actions']\n",
        "        rewards = batch['rewards']\n",
        "        next_features, next_adjacency, next_target_features, next_target_adjacency, next_mask, _ = batch['next_state']\n",
        "        dones = batch['dones']\n",
        "\n",
        "        # Get predictions for current state\n",
        "        local_logits, edge_logits, values = self.model(\n",
        "            features, adjacency, target_features, target_adjacency, mask, n_nodes[0])\n",
        "\n",
        "        # Get next state values\n",
        "        with torch.no_grad():\n",
        "            _, _, next_values = self.model(\n",
        "                next_features, next_adjacency, next_target_features, next_target_adjacency, next_mask, n_nodes[0])\n",
        "            # Make sure rewards and dones have the right shape for broadcasting\n",
        "            rewards = rewards.unsqueeze(1)\n",
        "            dones = dones.unsqueeze(1)\n",
        "            target_values = rewards + (1 - dones) * self.gamma * next_values\n",
        "\n",
        "        # Value loss (critic)\n",
        "        value_loss = F.mse_loss(values, target_values)\n",
        "\n",
        "        # Action loss (actor)\n",
        "        policy_loss = torch.zeros(1, device=self.device)\n",
        "\n",
        "        for i, (action_type, param) in enumerate(actions):\n",
        "            # Convert advantage to scalar with .item()\n",
        "            advantage = (target_values[i] - values[i]).detach().item()\n",
        "\n",
        "            if action_type == \"local\":\n",
        "                # Local complementation\n",
        "                node = param\n",
        "                policy_loss -= local_logits[i, node] * advantage\n",
        "            else:  # \"edge\"\n",
        "                # Edge toggle\n",
        "                src, dst = param\n",
        "                policy_loss -= edge_logits[i, src, dst] * advantage\n",
        "\n",
        "        # Total loss\n",
        "        loss = value_loss + policy_loss\n",
        "\n",
        "        # Update model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    # Fast learning method placeholder - if you have an optimized version\n",
        "    def _learn_fast(self):\n",
        "        return self._learn()\n",
        "\n",
        "    def train(self, env, num_episodes=1000, max_time=3600):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        start_time = time.time()\n",
        "        rewards_history = []\n",
        "        best_solution = None\n",
        "        best_solution_cost = float('inf')\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            if time.time() - start_time > max_time:\n",
        "                print(f\"Time limit reached after {episode} episodes.\")\n",
        "                break\n",
        "\n",
        "            # Reset environment\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # Select action\n",
        "                action = self.select_action(env, *state)\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                # Store experience\n",
        "                self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "                # Learn from experiences\n",
        "                if len(self.memory) > self.batch_size:\n",
        "                    self._learn()\n",
        "\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "            # Decay exploration rate\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            # Record metrics\n",
        "            rewards_history.append(episode_reward)\n",
        "\n",
        "            # Check if this solution is the best so far\n",
        "            if graph_difference(env.current_graph, env.target_graph) == 0:\n",
        "                solution_cost = calculate_cost(env.operations_history)\n",
        "                if solution_cost < best_solution_cost:\n",
        "                    best_solution = env.operations_history.copy()\n",
        "                    best_solution_cost = solution_cost\n",
        "                    print(f\"Episode {episode}: Found better solution with cost {best_solution_cost}\")\n",
        "\n",
        "            # Logging\n",
        "            if episode % 10 == 0:\n",
        "                avg_reward = sum(rewards_history[-10:]) / min(10, len(rewards_history[-10:]))\n",
        "                print(f\"Episode {episode}: Avg reward = {avg_reward:.2f}, Epsilon = {self.epsilon:.2f}\")\n",
        "                if best_solution:\n",
        "                    print(f\"Best solution so far: cost {best_solution_cost}, {len(best_solution)} operations\")\n",
        "\n",
        "        return best_solution, rewards_history\n",
        "\n",
        "# =========================================\n",
        "# VISUALIZATION\n",
        "# =========================================\n",
        "\n",
        "def visualize_transformation(initial_graph, operations, max_steps_to_show=12):\n",
        "    \"\"\"Visualize the graph transformation process step by step.\"\"\"\n",
        "    current = initial_graph.copy()\n",
        "\n",
        "    # Limit the number of steps to show\n",
        "    operations_to_show = operations[:max_steps_to_show]\n",
        "    n_steps = len(operations_to_show)\n",
        "\n",
        "    # Make sure we don't create an empty grid\n",
        "    if n_steps == 0:\n",
        "        # Just show the initial graph if no operations\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        pos = nx.spring_layout(current, seed=42)\n",
        "        nx.draw(current, pos, with_labels=True, node_color='lightblue',\n",
        "                node_size=500, font_weight='bold')\n",
        "        plt.title(\"Graph (No Operations)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return current\n",
        "\n",
        "    # Calculate an appropriate grid size for subplots\n",
        "    if n_steps <= 2:\n",
        "        rows, cols = 1, n_steps + 1  # One row: [initial] [op1] [op2]\n",
        "    elif n_steps <= 5:\n",
        "        rows, cols = 2, 3  # 2x3 grid (6 cells)\n",
        "    elif n_steps <= 8:\n",
        "        rows, cols = 3, 3  # 3x3 grid (9 cells)\n",
        "    else:\n",
        "        rows, cols = 4, 3  # 4x3 grid (12 cells)\n",
        "\n",
        "    # Create the figure\n",
        "    plt.figure(figsize=(cols*4, rows*4))\n",
        "\n",
        "    # Show initial graph\n",
        "    plt.subplot(rows, cols, 1)\n",
        "    pos = nx.spring_layout(current, seed=42)  # Consistent layout\n",
        "    nx.draw(current, pos, with_labels=True, node_color='lightblue',\n",
        "            node_size=500, font_weight='bold')\n",
        "    plt.title(\"Initial Graph\")\n",
        "\n",
        "    # Apply and show each operation\n",
        "    for i, (op_type, param) in enumerate(operations_to_show):\n",
        "        # Check if we've run out of subplot space\n",
        "        if i+2 > rows*cols:\n",
        "            print(f\"Warning: Too many operations to show. Showing only the first {i} operations.\")\n",
        "            break\n",
        "\n",
        "        # Apply operation\n",
        "        if op_type == \"edge\":\n",
        "            i_node, j_node = param\n",
        "            if current.has_edge(i_node, j_node):\n",
        "                current.remove_edge(i_node, j_node)\n",
        "                op_text = f\"Remove edge ({i_node}, {j_node})\"\n",
        "            else:\n",
        "                current.add_edge(i_node, j_node)\n",
        "                op_text = f\"Add edge ({i_node}, {j_node})\"\n",
        "        else:  # op_type == \"local\"\n",
        "            node = param\n",
        "            current = local_complementation(current, node)\n",
        "            op_text = f\"Local comp. on node {node}\"\n",
        "\n",
        "        plt.subplot(rows, cols, i+2)\n",
        "        nx.draw(current, pos, with_labels=True, node_color='lightgreen',\n",
        "                node_size=500, font_weight='bold')\n",
        "        plt.title(f\"Step {i+1}: {op_text}\")\n",
        "\n",
        "    # If we didn't show all operations, continue applying them without visualization\n",
        "    for op_type, param in operations[max_steps_to_show:]:\n",
        "        if op_type == \"edge\":\n",
        "            i, j = param\n",
        "            if current.has_edge(i, j):\n",
        "                current.remove_edge(i, j)\n",
        "            else:\n",
        "                current.add_edge(i, j)\n",
        "        else:  # op_type == \"local\"\n",
        "            node = param\n",
        "            current = local_complementation(current, node)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return current\n",
        "\n",
        "# =========================================\n",
        "# TRAINING FUNCTION\n",
        "# =========================================\n",
        "\n",
        "def train_agent(n_nodes=4, num_graphs=20, max_episodes=2000, target_success_rate=0.9):\n",
        "    \"\"\"\n",
        "    Train the RL agent on graph pairs with enhancements for finding optimal solutions.\n",
        "    \"\"\"\n",
        "    print(f\"Training on {num_graphs} randomly generated {n_nodes}-node graph pairs\")\n",
        "\n",
        "    # Generate more diverse graph pairs for training\n",
        "    graph_pairs = []\n",
        "\n",
        "    # Add predefined graph examples\n",
        "    G9 = nx.Graph()\n",
        "    G9.add_nodes_from(range(6))\n",
        "    G10 = nx.Graph()\n",
        "    G10.add_nodes_from(range(6))\n",
        "    G10.add_edges_from([(0, 1), (1, 2), (1, 3), (2, 3), (3, 5), (2, 4)])\n",
        "    graph_pairs.append((G9, G10))\n",
        "\n",
        "    G7 = nx.Graph()\n",
        "    G7.add_nodes_from(range(5))\n",
        "    G7.add_edges_from([(0, 1), (0, 2), (0, 3), (0, 4)])\n",
        "    G8 = nx.Graph()\n",
        "    G8.add_nodes_from(range(5))\n",
        "    G8.add_edges_from([(0, 1), (0, 2), (0, 3), (0, 4), (1, 4), (1, 2), (1, 3), (2, 3), (2, 4), (3, 4)])\n",
        "    graph_pairs.append((G7, G8))\n",
        "\n",
        "    G1 = nx.Graph()\n",
        "    G1.add_nodes_from(range(5))\n",
        "    G2 = nx.cycle_graph(5)\n",
        "    graph_pairs.append((G1, G2))\n",
        "\n",
        "    G3 = nx.Graph()\n",
        "    G3.add_nodes_from(range(8))\n",
        "    G4 = nx.cubical_graph(create_using=nx.Graph)\n",
        "    graph_pairs.append((G3, G4))\n",
        "\n",
        "    G5 = nx.Graph()\n",
        "    G5.add_nodes_from(range(6))\n",
        "    G6 = nx.Graph()\n",
        "    G6.add_nodes_from(range(6))\n",
        "    G6.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4), (3, 5), (1, 5)])\n",
        "    graph_pairs.append((G5, G6))\n",
        "\n",
        "    # Create agent\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    agent = GraphTransformAgent(max_nodes=n_nodes, device=device)\n",
        "\n",
        "    # Adjust agent parameters for better exploration\n",
        "    agent.epsilon = 1.0        # Start fully exploratory\n",
        "    agent.epsilon_decay = 0.998  # Slower decay for more exploration\n",
        "    agent.epsilon_min = 0.05   # Slightly higher minimum exploration\n",
        "\n",
        "    # Track training statistics\n",
        "    all_training_stats = []\n",
        "    overall_success_count = 0\n",
        "\n",
        "    # Train on all graph pairs\n",
        "    for i, (initial_graph, target_graph) in enumerate(graph_pairs):\n",
        "        print(f\"\\nTraining on graph pair {i+1}/{len(graph_pairs)}...\")\n",
        "\n",
        "        # Estimate theoretical minimum solution cost\n",
        "        est_min_cost = estimate_min_solution_cost(initial_graph, target_graph)\n",
        "        print(f\"Estimated minimum solution cost: {est_min_cost}\")\n",
        "\n",
        "        # Visualize the training pair\n",
        "        plt.figure(figsize=(10, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        pos = nx.spring_layout(initial_graph, seed=42)\n",
        "        nx.draw(initial_graph, pos, with_labels=True, node_color='lightblue',\n",
        "                node_size=500, font_weight='bold')\n",
        "        plt.title(\"Initial Graph\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        nx.draw(target_graph, pos, with_labels=True, node_color='lightgreen',\n",
        "                node_size=500, font_weight='bold')\n",
        "        plt.title(\"Target Graph\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        env = GraphTransformationEnv(initial_graph, target_graph)\n",
        "\n",
        "        # Track best solution for this graph pair\n",
        "        best_solution = None\n",
        "        best_solution_cost = float('inf')\n",
        "        success_found = False\n",
        "        consecutive_successes = 0  # Track consecutive successes for more reliable stopping\n",
        "\n",
        "        # Track different solution paths found\n",
        "        solution_archive = []  # List of (operations, cost) tuples\n",
        "\n",
        "        # Variables to control exploration phases\n",
        "        episodes_since_improvement = 0\n",
        "        exploration_phase = False\n",
        "\n",
        "        # Train until max episodes or success\n",
        "        for episode in range(max_episodes):\n",
        "            # Reset environment\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "\n",
        "            # Temporarily increase exploration if we're in exploration phase\n",
        "            if exploration_phase:\n",
        "                original_epsilon = agent.epsilon\n",
        "                agent.epsilon = 0.8  # High exploration\n",
        "\n",
        "            # Run episode\n",
        "            while not done:\n",
        "                action = agent.select_action(env, *state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                agent.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "                if len(agent.memory) > agent.batch_size:\n",
        "                    agent._learn_fast()\n",
        "\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "            # Reset epsilon if we were in exploration phase\n",
        "            if exploration_phase:\n",
        "                agent.epsilon = original_epsilon\n",
        "                exploration_phase = False\n",
        "\n",
        "            # Standard epsilon decay\n",
        "            if episode > 1000 and success_found:\n",
        "                agent.epsilon = max(0.01, agent.epsilon * 0.995)  # Lower floor, faster decay\n",
        "            else:\n",
        "                agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
        "\n",
        "            # Check if this episode found a solution\n",
        "            if graph_difference(env.current_graph, target_graph) == 0:\n",
        "                solution_cost = calculate_cost(env.operations_history)\n",
        "                success_found = True\n",
        "                consecutive_successes += 1\n",
        "\n",
        "                # Record different solution paths\n",
        "                solution_path = tuple((op, param if not isinstance(param, tuple) else tuple(param))\n",
        "                                      for op, param in env.operations_history)\n",
        "\n",
        "                # Check if this is a new solution path\n",
        "                if all(solution_path != path for path, _ in solution_archive):\n",
        "                    solution_archive.append((solution_path, solution_cost))\n",
        "                    print(f\"Episode {episode}: Found new solution path with cost {solution_cost}\")\n",
        "\n",
        "                # Check if this is a better solution\n",
        "                if solution_cost < best_solution_cost:\n",
        "                    best_solution = env.operations_history.copy()\n",
        "                    best_solution_cost = solution_cost\n",
        "                    print(f\"Episode {episode}: Found better solution with cost {best_solution_cost}\")\n",
        "                    episodes_since_improvement = 0\n",
        "                else:\n",
        "                    episodes_since_improvement += 1\n",
        "            else:\n",
        "                consecutive_successes = 0\n",
        "                episodes_since_improvement += 1\n",
        "\n",
        "            # Log progress\n",
        "            if episode % 50 == 0:\n",
        "                print(f\"Episode {episode}: Reward = {episode_reward:.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
        "                if best_solution:\n",
        "                    print(f\"Best solution so far: cost {best_solution_cost}, {len(best_solution)} operations\")\n",
        "\n",
        "            # Periodically enter exploration phase to find diverse solutions\n",
        "            if success_found and episode > 200 and episode % 100 == 0:\n",
        "                print(f\"Entering exploration phase at episode {episode}\")\n",
        "                exploration_phase = True\n",
        "\n",
        "            # If we've found a solution but it might not be optimal, try harder\n",
        "            if success_found and best_solution_cost > est_min_cost * 1.2 and episodes_since_improvement > 200:\n",
        "                print(f\"Solution cost ({best_solution_cost}) still above estimated minimum ({est_min_cost})\")\n",
        "                print(\"Temporarily increasing exploration to find better solutions...\")\n",
        "\n",
        "                # Save current epsilon\n",
        "                saved_epsilon = agent.epsilon\n",
        "                # Temporarily increase exploration\n",
        "                agent.epsilon = 0.9\n",
        "\n",
        "                # Run a set of focused exploration episodes\n",
        "                for _ in range(30):\n",
        "                    explore_state = env.reset()\n",
        "                    explore_done = False\n",
        "\n",
        "                    while not explore_done:\n",
        "                        explore_action = agent.select_action(env, *explore_state)\n",
        "                        explore_state, _, explore_done, _ = env.step(explore_action)\n",
        "\n",
        "                    # Check if we found a better solution\n",
        "                    if graph_difference(env.current_graph, target_graph) == 0:\n",
        "                        explore_cost = calculate_cost(env.operations_history)\n",
        "                        if explore_cost < best_solution_cost:\n",
        "                            best_solution = env.operations_history.copy()\n",
        "                            best_solution_cost = explore_cost\n",
        "                            print(f\"Exploration found better solution with cost {best_solution_cost}\")\n",
        "                            episodes_since_improvement = 0\n",
        "\n",
        "                # Restore epsilon\n",
        "                agent.epsilon = saved_epsilon\n",
        "\n",
        "            # Early stopping with verification\n",
        "            if success_found and episode > 200:\n",
        "                # Try 5 verification runs with epsilon=0 (no exploration)\n",
        "                verification_successes = 0\n",
        "                original_epsilon = agent.epsilon\n",
        "                agent.epsilon = 0\n",
        "\n",
        "                for _ in range(5):\n",
        "                    verify_state = env.reset()\n",
        "                    verify_done = False\n",
        "                    while not verify_done:\n",
        "                        verify_action = agent.select_action(env, *verify_state)\n",
        "                        verify_state, _, verify_done, _ = env.step(verify_action)\n",
        "\n",
        "                    if graph_difference(env.current_graph, target_graph) == 0:\n",
        "                        verification_successes += 1\n",
        "\n",
        "                agent.epsilon = original_epsilon\n",
        "\n",
        "                # Early stopping conditions:\n",
        "                # 1. If we can reliably solve it with a cost close to theoretical minimum\n",
        "                if verification_successes >= 4 and best_solution_cost <= est_min_cost * 1.2:\n",
        "                    print(f\"Solution verified in {verification_successes}/5 attempts!\")\n",
        "                    print(f\"Cost ({best_solution_cost}) is within 20% of estimated minimum ({est_min_cost})\")\n",
        "                    break\n",
        "\n",
        "                # 2. If we've had 3 consecutive successes and no improvement for a long time\n",
        "                if consecutive_successes >= 3 and episodes_since_improvement > 500:\n",
        "                    print(f\"Early stopping after {consecutive_successes} consecutive successes\")\n",
        "                    print(f\"No improvement for {episodes_since_improvement} episodes\")\n",
        "                    break\n",
        "\n",
        "        # Record training stats for this graph pair\n",
        "        if best_solution:\n",
        "            overall_success_count += 1\n",
        "            all_training_stats.append({\n",
        "                'graph_pair': i,\n",
        "                'success': True,\n",
        "                'episodes': episode + 1,\n",
        "                'best_cost': best_solution_cost,\n",
        "                'solution_length': len(best_solution),\n",
        "                'est_min_cost': est_min_cost,\n",
        "                'solution_archive': solution_archive\n",
        "            })\n",
        "\n",
        "            # Visualize the solution\n",
        "            print(\"\\nFinal solution:\")\n",
        "            visualize_transformation(initial_graph, best_solution)\n",
        "\n",
        "            # Show all discovered solutions\n",
        "            print(f\"\\nDiscovered {len(solution_archive)} different solution paths:\")\n",
        "            solution_archive.sort(key=lambda x: x[1])  # Sort by cost\n",
        "            for j, (solution_path, solution_cost) in enumerate(solution_archive[:5]):  # Show top 5\n",
        "                print(f\"Solution {j+1}: Cost {solution_cost}\")\n",
        "                ops = []\n",
        "                for op_type, param in solution_path:\n",
        "                    if op_type == \"local\":\n",
        "                        ops.append(f\"LC on node {param}\")\n",
        "                    else:\n",
        "                        i, j = param\n",
        "                        ops.append(f\"{'Add' if not initial_graph.has_edge(i, j) else 'Remove'} edge ({i},{j})\")\n",
        "                print(\" → \".join(ops))\n",
        "\n",
        "        else:\n",
        "            all_training_stats.append({\n",
        "                'graph_pair': i,\n",
        "                'success': False,\n",
        "                'episodes': episode + 1,\n",
        "                'best_cost': float('inf'),\n",
        "                'solution_length': 0,\n",
        "                'est_min_cost': est_min_cost\n",
        "            })\n",
        "            print(\"No solution found for this graph pair.\")\n",
        "\n",
        "    # Print overall training results\n",
        "    success_rate = overall_success_count / len(graph_pairs)\n",
        "    print(f\"\\nTraining completed:\")\n",
        "    print(f\"Success rate: {success_rate:.2f} ({overall_success_count}/{len(graph_pairs)} graph pairs)\")\n",
        "\n",
        "    if overall_success_count > 0:\n",
        "        avg_cost = sum(stat['best_cost'] for stat in all_training_stats if stat['success']) / overall_success_count\n",
        "        print(f\"Average solution cost: {avg_cost:.2f}\")\n",
        "\n",
        "        # Calculate optimality ratio\n",
        "        optimality_stats = [(stat['best_cost'] / stat['est_min_cost'])\n",
        "                           for stat in all_training_stats if stat['success']]\n",
        "        avg_optimality = sum(optimality_stats) / len(optimality_stats)\n",
        "        print(f\"Average solution optimality: {avg_optimality:.2f}x estimated minimum\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model_dir = 'models'\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    model_path = os.path.join(model_dir, f'graph_transform_model_{n_nodes}nodes.pt')\n",
        "    torch.save({\n",
        "        'model_state_dict': agent.model.state_dict(),\n",
        "        'n_nodes': n_nodes,\n",
        "        'training_stats': all_training_stats,\n",
        "        'success_rate': success_rate\n",
        "    }, model_path)\n",
        "\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "# =========================================\n",
        "# NOTEBOOK-COMPATIBLE MAIN FUNCTION\n",
        "# =========================================\n",
        "\n",
        "def run_notebook_training(n_nodes=8, num_graphs=5, max_episodes=2000):\n",
        "    \"\"\"Run training directly from a notebook cell.\"\"\"\n",
        "    trained_agent = train_agent(n_nodes=n_nodes, num_graphs=num_graphs, max_episodes=max_episodes)\n",
        "    return trained_agent\n",
        "\n",
        "def run_notebook_testing(model_path, initial_graph=None, target_graph=None):\n",
        "    \"\"\"Run testing on specific graphs from a notebook cell.\"\"\"\n",
        "    # Load the model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    agent = GraphTransformAgent(max_nodes=checkpoint['n_nodes'], device=device)\n",
        "    agent.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    agent.epsilon = 0  # No exploration during testing\n",
        "\n",
        "    # Create graphs if not provided\n",
        "    if initial_graph is None or target_graph is None:\n",
        "        initial_graph, target_graph = create_state_pairs(checkpoint['n_nodes'])\n",
        "\n",
        "    # Visualize graphs\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    pos = nx.spring_layout(initial_graph, seed=42)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    nx.draw(initial_graph, pos, with_labels=True, node_color='lightblue',\n",
        "            node_size=500, font_weight='bold')\n",
        "    plt.title(\"Initial Graph\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    nx.draw(target_graph, pos, with_labels=True, node_color='lightcoral',\n",
        "            node_size=500, font_weight='bold')\n",
        "    plt.title(\"Target Graph\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Run agent on graphs\n",
        "    env = GraphTransformationEnv(initial_graph, target_graph)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    max_steps = initial_graph.number_of_nodes() * 3\n",
        "\n",
        "    print(\"Finding solution...\")\n",
        "    while not done and steps < max_steps:\n",
        "        action = agent.select_action(env, *state)\n",
        "        state, _, done, _ = env.step(action)\n",
        "        steps += 1\n",
        "\n",
        "    # Check result\n",
        "    if graph_difference(env.current_graph, target_graph) == 0:\n",
        "        print(\"✅ Solution found!\")\n",
        "        cost = calculate_cost(env.operations_history)\n",
        "        print(f\"Solution cost: {cost}\")\n",
        "        print(f\"Number of steps: {steps}\")\n",
        "\n",
        "        # Display the solution\n",
        "        visualize_transformation(initial_graph, env.operations_history)\n",
        "        return env.operations_history\n",
        "    else:\n",
        "        print(\"❌ Failed to find solution\")\n",
        "        print(f\"Remaining difference: {graph_difference(env.current_graph, target_graph)}\")\n",
        "        return None\n",
        "\n",
        "# For running directly\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in notebook environment\n",
        "    try:\n",
        "        import IPython\n",
        "        is_notebook = True\n",
        "    except ImportError:\n",
        "        is_notebook = False\n",
        "\n",
        "    if is_notebook:\n",
        "        print(\"Running in notebook environment\")\n",
        "        # Call run_notebook_training() or run_notebook_testing() directly in cells\n",
        "    else:\n",
        "        # Command-line execution\n",
        "        import argparse\n",
        "        parser = argparse.ArgumentParser(description='Graph State Preparation using RL')\n",
        "        parser.add_argument('--mode', choices=['train', 'test'], default='train',\n",
        "                            help='Program mode: train or test')\n",
        "        parser.add_argument('--nodes', type=int, default=8,\n",
        "                            help='Number of nodes in training graphs')\n",
        "        parser.add_argument('--episodes', type=int, default=2000,\n",
        "                            help='Number of training episodes')\n",
        "        parser.add_argument('--model', type=str, default='models/graph_transform_model.pt',\n",
        "                            help='Path for saving/loading the model')\n",
        "\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        if args.mode == 'train':\n",
        "            train_agent(n_nodes=args.nodes, num_graphs=5, max_episodes=args.episodes)\n",
        "        else:\n",
        "            # This is a placeholder - proper testing would require more code for graph input\n",
        "            print(\"Test mode requires notebook environment or custom graphs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP_0tIrJkJAm",
        "outputId": "cce2af5b-6088-404d-be91-6e6883364cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in notebook environment\n"
          ]
        }
      ]
    }
  ]
}